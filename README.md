# travel_spider

## 背景介绍

因学术论文需要游记文本数据集，故开发此程序爬取携程网站的游记文本，数据只做学术研究，绝不商用。

## 程序启动

`python spider_main.py`

## 程序结构介绍

### db_config.json

数据库配置文件。这里可以配置想要连接的数据库

### place.json

这里定义一些你要爬取的地方的游记。<\br>

region表示你要爬取的区域是华东、华南、华北、西北还是西南等。<\br>

place表示你要爬取的旅游胜地。<\br>

place_url表示首页要爬取的链接。<\br>

place_url_page表示翻页的链接。<\br>

按照 json 格式定义好这些，爬虫程序会自动解析。<\br>

### spider_main.py

程序运行的主要入口，运行这个文件即可开始爬虫的工作。

### TravelSpider.py

爬虫解析程序。它会解析网页链接，提取相应的元素，比如提取网页中的游记标题、发表时间、游记文本内容等

### MysqlUtils.py

数据库工具类。定义了连接数据库、操作数据库等方法。

### Utils.py

工具类。定义一些通用方法的工具类，比如查找文件等方法定义在此。

## 程序可提升的地方

### 1.数据库连接池

可以使用数据库连接池，提升数据库的使用效率。同时可以采用批量插入的方式进行提升，而不是一条条插入。

### 2.多线程

目前使用的是单线程爬取，效率比较低，可以考虑使用多线程的方式进行提升。既然采用了多线程，同样可以考虑使用线程池的方式进行池化，减少资源的消耗，提高资源的利用率

### 3.设计模式

爬虫程序和解析程序符合生产者消费者模式。<\br>

爬虫程序是生产者，负责生产，生产的是url链接。<\br>

解析程序是消费者，负责消费，消费url链接，解析出文本内容，写入数据库。<\br>

所以这个程序是可以利用生产者消费者模式进行改造，进行提升代码的可读性，提升程序的效率。

### 4.中间件

既然采用了生产者消费者模式，那么中间的消息队列由谁来做支撑呢？此时的消息队列可以引入Kafka或者Redis作为中间的消息队列。<\br>



引入消息队列的作用：<\br>

解耦和扩展性：将生产者和消费者解耦开来，可以独立地扩展他们相应的功能。<\br>

缓冲和削峰：如果上游数据突然暴增，下游可能扛不住。所有可以通过消息队列进行缓冲，下游队列可以慢慢从消息队列取数据处理。<\br>

异步处理：有时候并不需要立即处理数据，可以通过消息队列的异步处理机制，它允许用户把消息放入消息队列里，但不立即处理它。



